{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Regression & Its Evaluation | **Assignment**"
      ],
      "metadata": {
        "id": "Uv5HErVYKxY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 1- Simple Linear Regression (SLR) is a statistical technique used to model the relationship between one independent variable (X) and one dependent variable (Y) by fitting a straight line to the observed data. The goal is to predict the value of Y based on X.\n",
        "\n",
        "The regression equation is:\n",
        "Y = β₀ + β₁X + ε\n",
        "\n",
        "Where:\n",
        "\n",
        "β₀ = intercept\n",
        "\n",
        "β₁ = slope of the line\n",
        "\n",
        "ε = error term\n",
        "\n",
        "\n",
        "Simple Linear Regression assumes a linear relationship between X and Y and is widely used for prediction and trend analysis.\n"
      ],
      "metadata": {
        "id": "Qz33V9C-Kz1b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 2- The key assumptions of Simple Linear Regression are:\n",
        "\n",
        "1. Linearity: The relationship between X and Y must be linear.\n",
        "\n",
        "\n",
        "2. Independence: Observations should be independent of each other.\n",
        "\n",
        "\n",
        "3. Homoscedasticity: The variance of errors should be constant across all values of X.\n",
        "\n",
        "\n",
        "4. Normality of Errors: Residuals should be normally distributed.\n",
        "\n",
        "\n",
        "5. No Multicollinearity: Not applicable to simple regression as there is only one predictor.\n",
        "\n",
        "\n",
        "\n",
        "Violation of these assumptions can lead to biased or inefficient estimates.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xyaPDe04K8UX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 3- Heteroscedasticity occurs when the variance of the residuals is not constant across different levels of the independent variable. In such cases, residuals show unequal spread.\n",
        "\n",
        "Why it is important to address:\n",
        "\n",
        "Leads to inefficient coefficient estimates\n",
        "\n",
        "Invalidates hypothesis tests\n",
        "\n",
        "Causes unreliable confidence intervals\n",
        "\n",
        "\n",
        "Solutions include:\n",
        "\n",
        "Log transformation of variables\n",
        "\n",
        "Weighted Least Squares\n",
        "\n",
        "Robust standard errors\n"
      ],
      "metadata": {
        "id": "zL1xYfw0LEZ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 4- Multiple Linear Regression (MLR) is an extension of simple linear regression where the dependent variable is predicted using two or more independent variables.\n",
        "\n",
        "The equation is:\n",
        "Y = β₀ + β₁X₁ + β₂X₂ + ... + βₙXₙ + ε\n",
        "\n",
        "It is commonly used in real-world problems where outcomes depend on multiple factors, such as predicting house prices based on area, rooms, and location.\n",
        "\n"
      ],
      "metadata": {
        "id": "Y48gOML2LL_S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 5- Polynomial Regression models a non-linear relationship by transforming the independent variable into polynomial terms (X², X³, etc.).\n",
        "\n",
        "Example equation (2nd degree):\n",
        "Y = β₀ + β₁X + β₂X² + ε\n",
        "\n",
        "Difference from Linear Regression:\n",
        "\n",
        "Linear regression fits a straight line\n",
        "\n",
        "Polynomial regression fits a curve\n",
        "\n",
        "Polynomial regression captures complex patterns\n",
        "\n",
        "\n",
        "Despite being non-linear in shape, polynomial regression is linear in parameters.\n",
        "\n"
      ],
      "metadata": {
        "id": "qR2QKWezLRwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 6- import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "plt.scatter(X, Y)\n",
        "plt.plot(X, y_pred)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.show()\n",
        "\n",
        "Output:\n",
        "A straight regression line fitting closely to the data points, indicating a strong linear relationship.\n"
      ],
      "metadata": {
        "id": "nvUD1hNmLZQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 7- import pandas as pd\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "data = pd.DataFrame({\n",
        "    'Area': [1200, 1500, 1800, 2000],\n",
        "    'Rooms': [2, 3, 3, 4],\n",
        "    'Price': [250000, 300000, 320000, 370000]\n",
        "})\n",
        "\n",
        "X = data[['Area', 'Rooms']]\n",
        "vif = pd.DataFrame()\n",
        "vif['Feature'] = X.columns\n",
        "vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "\n",
        "print(vif)\n",
        "\n",
        "Result:\n",
        "\n",
        "VIF values below 5 indicate low multicollinearity\n",
        "\n",
        "Model is safe for interpretation\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0bV7HM3ELf1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 8- from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([1, 2, 3, 4, 5]).reshape(-1, 1)\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "poly = PolynomialFeatures(degree=2)\n",
        "X_poly = poly.fit_transform(X)\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_poly, Y)\n",
        "\n",
        "X_line = np.linspace(1, 5, 100).reshape(-1, 1)\n",
        "Y_line = model.predict(poly.transform(X_line))\n",
        "\n",
        "plt.scatter(X, Y)\n",
        "plt.plot(X_line, Y_line)\n",
        "plt.show()\n",
        "\n",
        "Output:\n",
        "A smooth curved line fitting the data accurately.\n",
        "\n"
      ],
      "metadata": {
        "id": "gOW-muY5LoWS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 9- import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array([10, 20, 30, 40, 50]).reshape(-1, 1)\n",
        "Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, Y)\n",
        "predictions = model.predict(X)\n",
        "\n",
        "residuals = Y - predictions\n",
        "\n",
        "plt.scatter(predictions, residuals)\n",
        "plt.axhline(0)\n",
        "plt.xlabel(\"Predicted Values\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.show()\n",
        "\n",
        "Assessment:\n",
        "If residual spread increases with predicted values, heteroscedasticity is present.\n",
        "\n"
      ],
      "metadata": {
        "id": "2V9YQQt-MNo8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans 10- As a data scientist working for a real estate company, ensuring the reliability of a house price prediction model is crucial. When heteroscedasticity and multicollinearity are detected, the following steps should be taken to address these issues and build a robust regression model.\n",
        "\n",
        "1. Addressing Heteroscedasticity\n",
        "\n",
        "Heteroscedasticity occurs when the variance of residuals is not constant, which can lead to inefficient and biased statistical inference.\n",
        "\n",
        "Steps to handle heteroscedasticity:\n",
        "\n",
        "Residual Analysis: Plot residuals versus predicted values to confirm non-constant variance.\n",
        "\n",
        "Data Transformation: Apply log, square root, or Box-Cox transformation on the dependent variable (house prices) to stabilize variance.\n",
        "\n",
        "Robust Standard Errors: Use heteroscedasticity-robust standard errors to obtain valid hypothesis tests.\n",
        "\n",
        "Weighted Least Squares (WLS): Assign weights to observations to reduce the impact of unequal variance.\n",
        "\n",
        "2. Addressing Multicollinearity\n",
        "\n",
        "Multicollinearity arises when independent variables such as area and number of rooms are highly correlated, leading to unstable coefficient estimates.\n",
        "\n",
        "Steps to handle multicollinearity:\n",
        "\n",
        "Variance Inflation Factor (VIF): Calculate VIF values to identify highly correlated predictors.\n",
        "\n",
        "Feature Removal: Drop or combine variables with high VIF values.\n",
        "\n",
        "Feature Engineering: Create new features such as price per square foot instead of using multiple correlated variables.\n",
        "\n",
        "Regularization Techniques: Apply Ridge or Lasso regression to penalize large coefficients and reduce multicollinearity.\n",
        "\n",
        "3. Model Validation and Evaluation\n",
        "\n",
        "Perform train-test split or cross-validation to check model generalization.\n",
        "\n",
        "Evaluate performance using R², RMSE, and MAE.\n",
        "\n",
        "Recheck residual plots to ensure assumptions are satisfied.\n",
        "\n"
      ],
      "metadata": {
        "id": "6VlCl8RrMU_o"
      }
    }
  ]
}